{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbc7688-945f-4cce-a29e-223666b4970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No class subfolders found. Searching for CSV labels...\n",
      "[INFO] Mapped 6392 / 7000 images via CSV (C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\archive\\Odir5k preprocessed with CLAHE\\full_df.csv)\n",
      "[WARN] 608 images without a CSV label -> 'unknown'\n",
      "[INFO] Label map: {\"['a']\": 0, \"['c']\": 1, \"['d']\": 2, \"['g']\": 3, \"['h']\": 4, \"['m']\": 5, \"['n']\": 6, \"['o']\": 7, 'unknown': 8}\n",
      "[INFO] Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 7000/7000 [03:07<00:00, 37.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] X shape: (7000, 16) | samples: 7000 | classes: 9\n",
      "\n",
      "[SUMMARY]\n",
      "Artifacts created in: C:\\Users\\NXTWAVE\\Downloads\\Vision Care\n",
      " - HDF5: C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\visioncare_index.h5\n",
      " - PKL : C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\visioncare_index.pkl\n",
      " - YAML: C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\visioncare_index.yaml\n",
      " - JSON: C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\visioncare_index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time, glob, traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2, h5py, yaml\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# ========= EDIT THESE TWO PATHS IF NEEDED =========\n",
    "ROOT_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Vision Care\\archive\\Odir5k preprocessed with CLAHE\\training images\"\n",
    "OUT_DIR  = r\"C:\\Users\\NXTWAVE\\Downloads\\Vision Care\"\n",
    "# ==================================================\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# Feature params\n",
    "IMG_SIZE   = (256, 256)     # (w,h)\n",
    "LBP_P, LBP_R, LBP_METHOD = 8, 1, \"uniform\"  # uniform-> P+2 bins\n",
    "\n",
    "@dataclass\n",
    "class Item:\n",
    "    id: str\n",
    "    path: str\n",
    "    label_name: str\n",
    "    label_id: int\n",
    "    width: Optional[int] = None\n",
    "    height: Optional[int] = None\n",
    "    extra: Optional[dict] = None\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def list_subdirs(path: str) -> List[str]:\n",
    "    return [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "\n",
    "def list_files_recursive(dir_path: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, fnames in os.walk(dir_path):\n",
    "        for f in fnames:\n",
    "            files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "def is_image(path: str) -> bool:\n",
    "    return os.path.splitext(path)[1].lower() in IMG_EXTS\n",
    "\n",
    "def imread_color_unicode(path: str):\n",
    "    arr = np.fromfile(path, dtype=np.uint8)\n",
    "    if arr.size == 0: return None\n",
    "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "def resize_letterbox(img, target_wh):\n",
    "    tw, th = target_wh\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(tw / w, th / h)\n",
    "    nw, nh = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "    canvas = np.zeros((th, tw, 3), dtype=img.dtype)\n",
    "    y0, x0 = (th - nh)//2, (tw - nw)//2\n",
    "    canvas[y0:y0+nh, x0:x0+nw] = resized\n",
    "    return canvas\n",
    "\n",
    "def lbp_hist(gray):\n",
    "    lbp = local_binary_pattern(gray, LBP_P, LBP_R, LBP_METHOD)\n",
    "    n_bins = LBP_P + 2 if LBP_METHOD == \"uniform\" else int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist.astype(\"float32\")\n",
    "\n",
    "def extract_features(img_bgr):\n",
    "    img = resize_letterbox(img_bgr, IMG_SIZE)\n",
    "    means = img.mean(axis=(0,1))\n",
    "    stds  = img.std(axis=(0,1)) + 1e-8\n",
    "    color_stats = np.concatenate([means, stds], axis=0).astype(\"float32\")\n",
    "    green = img[:, :, 1]\n",
    "    lbp = lbp_hist(green)\n",
    "    return np.concatenate([lbp, color_stats], axis=0).astype(\"float32\")\n",
    "\n",
    "def collect_images_flat(root_dir: str) -> List[str]:\n",
    "    # images directly under root (not in subfolders)\n",
    "    candidates = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if is_image(os.path.join(root_dir, f))]\n",
    "    if candidates:\n",
    "        return candidates\n",
    "    # or anywhere under root (if nested)\n",
    "    return [p for p in list_files_recursive(root_dir) if is_image(p)]\n",
    "\n",
    "def try_load_labels_from_csv(root_dir: str) -> Optional[pd.DataFrame]:\n",
    "    # search for CSVs in ROOT_DIR and its parent\n",
    "    candidates = glob.glob(os.path.join(root_dir, \"*.csv\")) + glob.glob(os.path.join(os.path.dirname(root_dir), \"*.csv\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Known ODIR code columns\n",
    "    ODIR_CODES = [\"N\",\"D\",\"G\",\"C\",\"A\",\"H\",\"M\",\"O\"]\n",
    "    def primary_from_codes(row):\n",
    "        for code in ODIR_CODES:  # priority order as listed\n",
    "            if code in row and str(row[code]).strip() not in (\"\", \"0\", \"0.0\", \"False\", \"false\"):\n",
    "                try:\n",
    "                    if float(row[code]) > 0: \n",
    "                        return code\n",
    "                except:\n",
    "                    # if it's non-numeric but truthy\n",
    "                    return code\n",
    "        return None\n",
    "\n",
    "    for csv_path in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        cols = [c.lower() for c in df.columns]\n",
    "        df.columns = cols\n",
    "\n",
    "        # Try common patterns\n",
    "        # 1) filename + label\n",
    "        if \"filename\" in cols and \"label\" in cols:\n",
    "            out = df[[\"filename\",\"label\"]].copy()\n",
    "            out[\"filename\"] = out[\"filename\"].astype(str)\n",
    "            out[\"label\"] = out[\"label\"].astype(str)\n",
    "            out[\"__source_csv__\"] = csv_path\n",
    "            return out\n",
    "\n",
    "        # 2) id + label\n",
    "        if \"id\" in cols and \"label\" in cols:\n",
    "            out = df[[\"id\",\"label\"]].copy().rename(columns={\"id\":\"filename\"})\n",
    "            out[\"filename\"] = out[\"filename\"].astype(str)\n",
    "            out[\"label\"] = out[\"label\"].astype(str)\n",
    "            out[\"__source_csv__\"] = csv_path\n",
    "            return out\n",
    "\n",
    "        # 3) filename + diagnosis / labels (single text col)\n",
    "        for txtcol in (\"diagnosis\",\"labels\",\"label_text\",\"disease\",\"diseases\"):\n",
    "            if \"filename\" in cols and txtcol in cols:\n",
    "                out = df[[\"filename\", txtcol]].copy().rename(columns={txtcol:\"label\"})\n",
    "                out[\"filename\"] = out[\"filename\"].astype(str)\n",
    "                out[\"label\"] = out[\"label\"].astype(str)\n",
    "                out[\"__source_csv__\"] = csv_path\n",
    "                return out\n",
    "\n",
    "        # 4) ODIR multi-hot columns N,D,G,C,A,H,M,O\n",
    "        if all(code.lower() in cols for code in [c.lower() for c in [\"N\",\"D\",\"G\",\"C\",\"A\",\"H\",\"M\",\"O\"]]):\n",
    "            name_col = \"filename\" if \"filename\" in cols else (\"id\" if \"id\" in cols else None)\n",
    "            if name_col is None:\n",
    "                # try to guess a column holding filenames by searching for .jpg/.png patterns\n",
    "                for c in df.columns:\n",
    "                    if df[c].astype(str).str.contains(r\"\\.(jpg|jpeg|png|bmp|tif|tiff)$\", case=False, regex=True).any():\n",
    "                        name_col = c\n",
    "                        break\n",
    "            if name_col:\n",
    "                df_codes = df.copy()\n",
    "                df_codes[\"__primary__\"] = df_codes.apply(primary_from_codes, axis=1)\n",
    "                out = df_codes[[name_col, \"__primary__\"]].rename(columns={name_col:\"filename\",\"__primary__\":\"label\"}).copy()\n",
    "                out[\"filename\"] = out[\"filename\"].astype(str)\n",
    "                out[\"label\"] = out[\"label\"].fillna(\"unknown\").astype(str)\n",
    "                out[\"__source_csv__\"] = csv_path\n",
    "                return out\n",
    "\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    assert os.path.isdir(ROOT_DIR), f\"Folder not found: {ROOT_DIR}\"\n",
    "    ensure_dir(OUT_DIR)\n",
    "\n",
    "    # 1) Detect class subfolders\n",
    "    class_dirs = list_subdirs(ROOT_DIR)\n",
    "    use_subfolders = len(class_dirs) > 0\n",
    "\n",
    "    # 2) Gather records: (path, label_name)\n",
    "    records = []\n",
    "    if use_subfolders:\n",
    "        # subfolders as labels\n",
    "        class_dirs_sorted = sorted(class_dirs, key=lambda p: os.path.basename(p).lower())\n",
    "        for cdir in class_dirs_sorted:\n",
    "            cname = os.path.basename(cdir).lower()\n",
    "            imgs = [p for p in list_files_recursive(cdir) if is_image(p)]\n",
    "            for p in imgs:\n",
    "                records.append((p, cname))\n",
    "        print(f\"[INFO] Using {len(class_dirs_sorted)} class subfolders as labels.\")\n",
    "    else:\n",
    "        # flat folder; try to use CSV mapping\n",
    "        print(\"[INFO] No class subfolders found. Searching for CSV labels...\")\n",
    "        df_lbl = try_load_labels_from_csv(ROOT_DIR)\n",
    "        imgs = collect_images_flat(ROOT_DIR)\n",
    "        assert imgs, f\"No images found in {ROOT_DIR}\"\n",
    "\n",
    "        if df_lbl is None:\n",
    "            # last resort: everyone is 'unknown'\n",
    "            print(\"[WARN] No CSV labels found. Assigning label 'unknown' to all images.\")\n",
    "            for p in imgs:\n",
    "                records.append((p, \"unknown\"))\n",
    "        else:\n",
    "            # index by stem name (filename without extension) and also full filename\n",
    "            df_lbl[\"filename\"] = df_lbl[\"filename\"].astype(str)\n",
    "            lbl_map_stem = {os.path.splitext(os.path.basename(a))[0].lower(): str(b) for a,b in zip(df_lbl[\"filename\"], df_lbl[\"label\"])}\n",
    "            lbl_map_full = {os.path.basename(a).lower(): str(b) for a,b in zip(df_lbl[\"filename\"], df_lbl[\"label\"])}\n",
    "            miss = 0\n",
    "            for p in imgs:\n",
    "                base = os.path.basename(p)\n",
    "                stem = os.path.splitext(base)[0].lower()\n",
    "                key_full = base.lower()\n",
    "                label = lbl_map_full.get(key_full, lbl_map_stem.get(stem, \"unknown\"))\n",
    "                if label == \"unknown\":\n",
    "                    miss += 1\n",
    "                records.append((p, str(label).strip().lower()))\n",
    "            print(f\"[INFO] Mapped {len(imgs)-miss} / {len(imgs)} images via CSV ({df_lbl.get('__source_csv__', ['?'])[0] if '__source_csv__' in df_lbl.columns else '?'})\")\n",
    "            if miss:\n",
    "                print(f\"[WARN] {miss} images without a CSV label -> 'unknown'\")\n",
    "\n",
    "    assert records, \"No labeled records found.\"\n",
    "\n",
    "    # 3) Build label map\n",
    "    label_names = sorted(list({lab for _, lab in records}))\n",
    "    label_to_id = {name: i for i, name in enumerate(label_names)}\n",
    "    print(\"[INFO] Label map:\", label_to_id)\n",
    "\n",
    "    # 4) Extract features\n",
    "    feats, labels, paths, widths, heights, items = [], [], [], [], [], []\n",
    "    print(\"[INFO] Extracting features...\")\n",
    "    for idx, (path, lname) in enumerate(tqdm(records)):\n",
    "        try:\n",
    "            img = imread_color_unicode(path)\n",
    "            if img is None: \n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            feat = extract_features(img)\n",
    "            feats.append(feat); labels.append(label_to_id[lname]); paths.append(path)\n",
    "            widths.append(w); heights.append(h)\n",
    "            items.append(Item(\n",
    "                id=str(len(paths)-1), path=path, label_name=lname, label_id=label_to_id[lname],\n",
    "                width=w, height=h, extra=None\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {path} -> {e}\")\n",
    "\n",
    "    assert len(feats) > 0, \"No features extracted. Check images.\"\n",
    "    X = np.stack(feats, axis=0)\n",
    "    y = np.array(labels, dtype=np.int64)\n",
    "    print(f\"[INFO] X shape: {X.shape} | samples: {len(items)} | classes: {len(label_names)}\")\n",
    "\n",
    "    # 5) Save artifacts\n",
    "    base = \"visioncare_index\"\n",
    "    ensure_dir(OUT_DIR)\n",
    "\n",
    "    # HDF5\n",
    "    h5_path = os.path.join(OUT_DIR, f\"{base}.h5\")\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        f.create_dataset(\"X\", data=X, compression=\"gzip\", compression_opts=4)\n",
    "        f.create_dataset(\"y\", data=y, compression=\"gzip\", compression_opts=4)\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        ds_paths = f.create_dataset(\"filenames\", (len(paths),), dtype=dt); ds_paths[:] = paths\n",
    "        f.attrs[\"img_size\"] = json.dumps({\"w\": IMG_SIZE[0], \"h\": IMG_SIZE[1]})\n",
    "        f.attrs[\"lbp\"] = json.dumps({\"P\": LBP_P, \"R\": LBP_R, \"method\": LBP_METHOD})\n",
    "        f.attrs[\"label_map\"] = json.dumps({str(i): name for name, i in label_to_id.items()})\n",
    "        f.attrs[\"feature_schema\"] = json.dumps({\n",
    "            \"order\": [\"lbp_green_hist\", \"rgb_means\", \"rgb_stds\"],\n",
    "            \"dims\": {\"lbp_green_hist\": (LBP_P + 2), \"rgb_means\": 3, \"rgb_stds\": 3},\n",
    "            \"total_dim\": int(X.shape[1])\n",
    "        })\n",
    "\n",
    "    # PKL\n",
    "    pkl_path = os.path.join(OUT_DIR, f\"{base}.pkl\")\n",
    "    df = pd.DataFrame(X)\n",
    "    df[\"label_id\"] = y\n",
    "    df[\"label_name\"] = [it.label_name for it in items]\n",
    "    df[\"path\"] = paths\n",
    "    df[\"width\"] = widths\n",
    "    df[\"height\"] = heights\n",
    "    df.to_pickle(pkl_path)\n",
    "\n",
    "    # YAML\n",
    "    yaml_path = os.path.join(OUT_DIR, f\"{base}.yaml\")\n",
    "    by_label_counts = defaultdict(int)\n",
    "    for it in items:\n",
    "        by_label_counts[it.label_name] += 1\n",
    "    stats = {\n",
    "        \"total_items\": int(len(items)),\n",
    "        \"classes\": int(len(label_names)),\n",
    "        \"by_label\": {name: int(by_label_counts[name]) for name in label_names},\n",
    "        \"img_size\": {\"width\": IMG_SIZE[0], \"height\": IMG_SIZE[1]},\n",
    "        \"lbp\": {\"P\": LBP_P, \"R\": LBP_R, \"method\": LBP_METHOD},\n",
    "        \"feature_schema\": {\n",
    "            \"order\": [\"lbp_green_hist\", \"rgb_means\", \"rgb_stds\"],\n",
    "            \"dims\": {\"lbp_green_hist\": (LBP_P + 2), \"rgb_means\": 3, \"rgb_stds\": 3},\n",
    "            \"total_dim\": int(X.shape[1])\n",
    "        },\n",
    "        \"label_map\": {name: int(idx) for name, idx in label_to_id.items()}\n",
    "    }\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as fy:\n",
    "        yaml.safe_dump(stats, fy, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    # JSON\n",
    "    json_path = os.path.join(OUT_DIR, f\"{base}.json\")\n",
    "    payload = {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"label_map\": {name: int(idx) for name, idx in label_to_id.items()},\n",
    "        \"items\": [asdict(it) for it in items]\n",
    "    }\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as fj:\n",
    "        json.dump(payload, fj, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    print(\"Artifacts created in:\", OUT_DIR)\n",
    "    print(\" - HDF5:\", h5_path)\n",
    "    print(\" - PKL :\", pkl_path)\n",
    "    print(\" - YAML:\", yaml_path)\n",
    "    print(\" - JSON:\", json_path)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d602601-37c9-4fec-980d-a6f4afaeab40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
